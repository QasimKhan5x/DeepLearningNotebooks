{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom keras.utils import to_categorical\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, random_split, DataLoader\nimport torchvision.transforms as T\nfrom torchvision import models","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lesion_type_dict = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'dermatofibroma',\n    'bkl': 'Benign keratosis-like lesions ',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}\nlesion_categorical = {k:i for i, k in enumerate(lesion_type_dict)}","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DATA_DIR = '../input/skin-cancer-mnist-ham10000'\noriginal_df = pd.read_csv(os.path.join(DATA_DIR, 'HAM10000_metadata.csv'))","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_df.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"     lesion_id      image_id   dx dx_type   age   sex localization\n0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0027419</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0025030</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0026769</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0025661</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0001466</td>\n      <td>ISIC_0031633</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>75.0</td>\n      <td>male</td>\n      <td>ear</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_df['localization'].unique()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"array(['scalp', 'ear', 'face', 'back', 'trunk', 'chest',\n       'upper extremity', 'abdomen', 'unknown', 'lower extremity',\n       'genital', 'neck', 'hand', 'foot', 'acral'], dtype=object)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"localization_to_index = {name:i for i, name in enumerate(original_df['localization'].unique())}","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_df['lesion_id'].nunique()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"7470"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop dx_type since we don't care about it when predicting dx\ntemp_df = original_df.drop('dx_type', axis = 1)\n# Convert dx to categorical\ntemp_df['dx'] = temp_df['dx'].map(lesion_categorical)\n# Drop null values\ntemp_df = temp_df.dropna()\ntemp_df.head()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"     lesion_id      image_id  dx   age   sex localization\n0  HAM_0000118  ISIC_0027419   2  80.0  male        scalp\n1  HAM_0000118  ISIC_0025030   2  80.0  male        scalp\n2  HAM_0002730  ISIC_0026769   2  80.0  male        scalp\n3  HAM_0002730  ISIC_0025661   2  80.0  male        scalp\n4  HAM_0001466  ISIC_0031633   2  75.0  male          ear","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0027419</td>\n      <td>2</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0025030</td>\n      <td>2</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0026769</td>\n      <td>2</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0025661</td>\n      <td>2</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0001466</td>\n      <td>ISIC_0031633</td>\n      <td>2</td>\n      <td>75.0</td>\n      <td>male</td>\n      <td>ear</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Perform upsampling\nThere is a huge class imbalance in the dataset. Let's first demonstrate it and then attempt to solve it by upsampling."},{"metadata":{"trusted":true},"cell_type":"code","source":"freqs = temp_df['dx'].value_counts()\nfreqs","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"0    6660\n1    1111\n2    1089\n3     514\n4     327\n5     142\n6     115\nName: dx, dtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The disease with index 0 has imbalanced the other diseases. Let's perform upsampling to solve this issue."},{"metadata":{"trusted":true},"cell_type":"code","source":"upsample_rate = [6705 // freq for freq in freqs]\n\nfor i in range(1, 7):\n    temp_df = temp_df.append([temp_df.loc[temp_df['dx'] == i,:]]*(upsample_rate[i]-1), ignore_index=True)\n    \ntemp_df['dx'].value_counts(normalize=True)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"3    0.143928\n5    0.143756\n6    0.143669\n1    0.143583\n0    0.143454\n4    0.140869\n2    0.140740\nName: dx, dtype: float64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Note\n**This step is optional. I found that only class 0 has duplicates. So if you perform it, your validation set will only contain one class, which is way worse than the problem we're trying to avoid. Think of a way to solve it and tell me; thanks!**\n\n## Creating Train and Validation Sets\nWe know that there are duplicate lesion id's in the dataset. This is because each lesion id maps to more than 1 image id. So, if we split right away, duplicates might end up in the train and validation set. To avoid this, perform the following steps:\n1. Create a column that indicates whether a lesion id has duplicates or not\n2. Create training set with rows that have duplicates\n3. Perform a random split on rows without duplicates\n4. The result of the split will be the validation set and another training set that will be concatenated to the original\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#duplicates = temp_df.groupby('lesion_id')['image_id'].count()\n#duplicates.head()\n#temp_df['has_duplicates'] = temp_df['lesion_id'].apply(\n#    lambda lesion_id: duplicates[lesion_id] > 1\n#)\n#temp_df['has_duplicates'].value_counts()\n#train_df = temp_df[temp_df['has_duplicates']]\n#mixed_df = temp_df[~temp_df['has_duplicates']]\n#train_df['dx'].value_counts(normalize=True)\n#val_df['dx'].value_counts(normalize=True)\n#mixed_df.shape[0], train_df.shape[0]\ntemp_df.shape[0]","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"46426"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Decide lengths of train and val dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, val_df = train_test_split(temp_df, test_size=0.2)\ntrain_df.shape[0], val_df.shape[0]","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"(37140, 9286)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df = pd.concat([train_df, train_temp], ignore_index=True)\n#train_df.shape[0], val_df.shape[0]","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the has_duplicates column since we don't need it anymore\n#train_df = train_df.drop('has_duplicates', axis = 1)\n#val_df = val_df.drop('has_duplicates', axis = 1).reset_index()","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.reset_index(inplace=True, drop=True)\nval_df.reset_index(inplace=True, drop=True)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"     lesion_id      image_id  dx   age     sex     localization\n0  HAM_0003110  ISIC_0032114   6  65.0    male  lower extremity\n1  HAM_0000005  ISIC_0024579   4  75.0  female  lower extremity\n2  HAM_0004423  ISIC_0026737   2  65.0  female  upper extremity\n3  HAM_0002243  ISIC_0033019   3  80.0    male             back\n4  HAM_0006371  ISIC_0033780   6  35.0  female  lower extremity","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0003110</td>\n      <td>ISIC_0032114</td>\n      <td>6</td>\n      <td>65.0</td>\n      <td>male</td>\n      <td>lower extremity</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000005</td>\n      <td>ISIC_0024579</td>\n      <td>4</td>\n      <td>75.0</td>\n      <td>female</td>\n      <td>lower extremity</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0004423</td>\n      <td>ISIC_0026737</td>\n      <td>2</td>\n      <td>65.0</td>\n      <td>female</td>\n      <td>upper extremity</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0002243</td>\n      <td>ISIC_0033019</td>\n      <td>3</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>back</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0006371</td>\n      <td>ISIC_0033780</td>\n      <td>6</td>\n      <td>35.0</td>\n      <td>female</td>\n      <td>lower extremity</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_df.head()","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"     lesion_id      image_id  dx   age   sex     localization\n0  HAM_0007089  ISIC_0030066   0  35.0  male            chest\n1  HAM_0005346  ISIC_0028171   0  75.0  male  lower extremity\n2  HAM_0004881  ISIC_0032913   1  85.0  male          abdomen\n3  HAM_0007051  ISIC_0031002   6  65.0  male  upper extremity\n4  HAM_0002787  ISIC_0025425   5  50.0  male            trunk","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0007089</td>\n      <td>ISIC_0030066</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>male</td>\n      <td>chest</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0005346</td>\n      <td>ISIC_0028171</td>\n      <td>0</td>\n      <td>75.0</td>\n      <td>male</td>\n      <td>lower extremity</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0004881</td>\n      <td>ISIC_0032913</td>\n      <td>1</td>\n      <td>85.0</td>\n      <td>male</td>\n      <td>abdomen</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0007051</td>\n      <td>ISIC_0031002</td>\n      <td>6</td>\n      <td>65.0</td>\n      <td>male</td>\n      <td>upper extremity</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0002787</td>\n      <td>ISIC_0025425</td>\n      <td>5</td>\n      <td>50.0</td>\n      <td>male</td>\n      <td>trunk</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Create Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/xinruizhuang/skin-lesion-classification-acc-90-pytorch\nmean = [0.763038, 0.54564667, 0.57004464]\nstd = [0.14092727, 0.15261286, 0.1699712]","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SkinCancerDataset(Dataset):\n    def __init__(self, df, transforms = None):\n        self.df = df\n        self.transforms = transforms\n        self.part1 = os.path.join(DATA_DIR, 'HAM10000_images_part_1')\n        self.part2 = os.path.join(DATA_DIR, 'HAM10000_images_part_2')\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.loc[idx]\n        image_id = row['image_id']\n        dx = row['dx']\n        features = list(row.loc['age':'localization'])\n        age, sex, localization = features[0], features[1], features[2]\n        sex = 0 if sex == 'male' else 1\n        localization = localization_to_index[localization]\n        sex = to_categorical(sex, num_classes = 2)\n        localization = to_categorical(localization, num_classes = len(localization_to_index))\n        \n        try:\n            img = Image.open(os.path.join(self.part1, image_id + '.jpg'))\n        except FileNotFoundError:\n            img = Image.open(os.path.join(self.part2, image_id + '.jpg'))\n            \n        if self.transforms != None:\n            img = self.transforms(img)\n\n        return  torch.tensor([age], dtype=torch.float32), torch.tensor(sex, dtype=torch.float32),\\\n                torch.tensor(localization, dtype=torch.float32), img, dx","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = get_default_device()\ndevice","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Modeling with Pretrained Networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature_extract is a boolean that defines if we are finetuning or feature extracting. \n# If feature_extract = False, the model is finetuned and all model parameters are updated. \n# If feature_extract = True, only the last layer parameters are updated, the others remain fixed.\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_model(model_name, num_classes, use_pretrained=True):\n    # Initialize these variables which will be set in this if statement. Each of these\n    #   variables is model specific.\n    \n    assert model_name in ['resnet', 'vgg', 'densenet', 'inception'], 'Invalid model name'\n\n    if model_name == \"resnet\":\n        \"\"\" Resnet18, resnet34, resnet50, resnet101 \"\"\"\n        model_ft = models.resnet50(pretrained=use_pretrained)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n\n    elif model_name == \"vgg\":\n        \"\"\" VGG11_bn \"\"\"\n        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n\n    elif model_name == \"densenet\":\n        \"\"\" Densenet121 \"\"\"\n        model_ft = models.densenet121(pretrained=use_pretrained)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    else:\n        \"\"\" Inception v3\n        Be careful, expects (299,299) sized images and has auxiliary output\n        \"\"\"\n        model_ft = models.inception_v3(pretrained=use_pretrained)\n        # Handle the auxilary net\n        num_ftrs = model_ft.AuxLogits.fc.in_features\n        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n        # Handle the primary net\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n        input_size = 299\n\n    return model_ft, input_size","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'densenet'\nmodel_ft, image_size = initialize_model(model_name, 20, use_pretrained=True)","execution_count":24,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=32342954.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4786c5eea71745f1964e7a9cff09f58a"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        age, sex, localization, image, label = batch\n        out = self(image, age, sex, localization) \n        loss = F.cross_entropy(out, label)      \n        return loss\n    \n    def validation_step(self, batch):\n        age, sex, localization, image, label = batch\n        out = self(image, age, sex, localization)\n        loss = F.cross_entropy(out, label)\n        score = F_score(out, label)\n        return {'val_loss': loss.detach(), 'val_score': score.detach() }\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_scores = [x['val_score'] for x in outputs]\n        epoch_score = torch.stack(batch_scores).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_score': epoch_score.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.4f}, train_loss: {:.4f}, val_loss: {:.4f}, val_score: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_score']))\n","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyModel(ImageClassificationBase):\n    def __init__(self, model_name):\n        super().__init__()\n        self.cnn = model_ft\n        self.fc1 = nn.Linear(20 + 1 + 2 + 15, 64)\n        self.fc2 = nn.Linear(64, 16)\n        self.fc3 = nn.Linear(16, 7)\n        \n    def forward(self, image, age, sex, localization):\n        x1 = self.cnn(image)\n        x = torch.cat([x1, age, sex, localization], dim=1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = SkinCancerDataset(train_df, transforms=T.Compose([\n                                                                T.RandomCrop(image_size, padding=8, padding_mode='reflect'),\n                                                                T.ToTensor(),\n                                                                T.Normalize(mean, std, inplace=True)\n                                                            ])\n                            )\nval_ds = SkinCancerDataset(val_df, transforms=T.Compose([\n                                                                T.RandomCrop(image_size, padding=8, padding_mode='reflect'),\n                                                                T.ToTensor(),\n                                                                T.Normalize(mean, std, inplace=True)\n                                                            ])\n                            )","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\n\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, \n                      num_workers=3, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, \n                    num_workers=2, pin_memory=True)\n\ntrain_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MyModel(model_name)\nmodel = model.to(device)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1\nimage = torch.randn(batch_size, 3, 224, 224).to(device)\nage = torch.randn(batch_size, 1).to(device)\nsex = torch.randn(batch_size, 2).to(device)\nlocalization = torch.randn(batch_size, 15).to(device)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model(image, age, sex, localization)","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"tensor([[ 0.0624, -0.2529, -0.2883, -0.0562, -0.2014,  0.0304,  0.4079]],\n       device='cuda:0', grad_fn=<AddmmBackward>)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Training Phase"},{"metadata":{"trusted":true},"cell_type":"code","source":"def F_score(output, label, beta=1):\n    _, prob = output.max(dim=1)\n    TP = (prob & label).sum().float()\n    TN = ((~prob) & (~label)).sum().float()\n    FP = (prob & (~label)).sum().float()\n    FN = ((~prob) & label).sum().float()\n\n    precision = torch.mean(TP / (TP + FP + 1e-12))\n    recall = torch.mean(TP / (TP + FN + 1e-12))\n    F2 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)\n    return F2.mean(0)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, freeze, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=optim.Adam):\n    model.train()\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    # Freeze or Unfreeze\n    set_parameter_requires_grad(model, freeze)\n    for epoch in range(epochs):\n        # Training Phase \n        \n        train_losses = []\n        lrs = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","execution_count":33,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"for batch in train_dl:\n    age, sex, localization, image, label = batch\n    res = model(image, age, sex, localization)\n    break"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train all layers\nmodel = MyModel(model_name)\nmodel = model.to(device)\nepochs = 10\nmax_lr = 3e-3\nfreeze = False\nhistory = fit_one_cycle(epochs, \n                        max_lr,\n                        model,\n                        freeze, \n                        train_dl, \n                        val_dl)","execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1161.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe16dcc1cb1d4d2ca0229f235aa5761d"}},"metadata":{}},{"output_type":"stream","text":"\nEpoch [0], last_lr: 0.0008, train_loss: 1.0046, val_loss: 0.9190, val_score: 0.8075\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1161.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a4859b8ce84483cbec4a5e3bd96dbf9"}},"metadata":{}},{"output_type":"stream","text":"\nEpoch [1], last_lr: 0.0023, train_loss: 1.2688, val_loss: 1.0065, val_score: 0.7717\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1161.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"093d29617e33499c8e7114160b6d4d6f"}},"metadata":{}},{"output_type":"stream","text":"\nEpoch [2], last_lr: 0.0030, train_loss: 1.0106, val_loss: 1.0170, val_score: 0.7711\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1161.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c7ea54117b3428cb8118af4ea51a702"}},"metadata":{}},{"output_type":"stream","text":"\nEpoch [3], last_lr: 0.0029, train_loss: 0.9355, val_loss: 0.8500, val_score: 0.8051\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1161.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61f61b0efe6345559b4bf1bbda1a4f51"}},"metadata":{}},{"output_type":"stream","text":"\nEpoch [4], last_lr: 0.0024, train_loss: 0.8464, val_loss: 0.8214, val_score: 0.8083\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1161.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82e58a10dc744563a6dcbf28d69b09d0"}},"metadata":{}},{"output_type":"stream","text":"\nEpoch [5], last_lr: 0.0018, train_loss: 0.7594, val_loss: 0.7725, val_score: 0.8218\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1161.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28c451f5c19a44708be86fc8799a2b33"}},"metadata":{}},{"output_type":"stream","text":"\nEpoch [6], last_lr: 0.0012, train_loss: 0.6640, val_loss: 0.6998, val_score: 0.8318\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1161.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c72b1f50d2f44e2f8e3a376a2324e957"}},"metadata":{}},{"output_type":"stream","text":"\nEpoch [7], last_lr: 0.0006, train_loss: 0.5562, val_loss: 0.5324, val_score: 0.8761\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1161.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3e120c89b104ff1bf2970b582d16d7a"}},"metadata":{}},{"output_type":"stream","text":"\nEpoch [8], last_lr: 0.0001, train_loss: 0.4706, val_loss: 0.4508, val_score: 0.9060\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1161.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92f165704fc347c1aae99ff78449b71a"}},"metadata":{}},{"output_type":"stream","text":"\nEpoch [9], last_lr: 0.0000, train_loss: 0.4313, val_loss: 0.4295, val_score: 0.9111\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train all layers\n#history += fit_one_cycle(epochs,\n#                         max_lr/10,\n#                         model,\n#                         freeze,\n#                         train_dl,\n#                         val_dl)","execution_count":35,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}